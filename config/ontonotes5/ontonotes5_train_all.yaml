### Model
model_name: bert-base-cased
dropout: 0
hidden_dim: 768

### Data
data_path: [datasets/NER_data/ontonotes5]
n_samples: -1
schema: BIO

### Training
batch_size: 8
max_seq_length: 512

lr: 2e-3  # last lr is 4e-4
stable_lr: 4e-4

schedule: (5,)
gamma: 0.2

first_training_epochs: 10
training_epochs: 10
evaluate_interval: 1
info_per_epochs: 1
early_stop: 100
